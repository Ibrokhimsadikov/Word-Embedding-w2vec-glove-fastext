{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statistics as st\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix, accuracy_score,classification_report\n",
    "import sklearn\n",
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "X_test, y_test = load_svmlight_file(\"Test_labeledBow.feat\")\n",
    "X_train, y_train = load_svmlight_file(\"Train_labeledBow.feat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse._sparsetools\n",
    "import scipy\n",
    "X_test_df=scipy.sparse.lil_matrix(X_test, dtype='uint16')\n",
    "X_train_df=scipy.sparse.lil_matrix(X_train, dtype='uint16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_df=X_test_df.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df=X_train_df.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df=X_train_df.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([138, 114, 115, ..., 231, 153, 138], dtype=uint32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2=np.where(X_train_df < 17, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df=pd.DataFrame(X_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>89517</th>\n",
       "      <th>89518</th>\n",
       "      <th>89519</th>\n",
       "      <th>89520</th>\n",
       "      <th>89521</th>\n",
       "      <th>89522</th>\n",
       "      <th>89523</th>\n",
       "      <th>89524</th>\n",
       "      <th>89525</th>\n",
       "      <th>89526</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 89527 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0      1      2      3      4      5      6      7      8      9      ...  \\\n",
       "0      9      1      4      4      6      4      2      2      4      0  ...   \n",
       "1      7      4      2      2      0      4      1      0      2      2  ...   \n",
       "2      4      4      4      7      2      1      1      1      0      1  ...   \n",
       "3     10      2      2      0      3      2      4      2      0      1  ...   \n",
       "4     13      9      6      4      2      5     10      6      0      2  ...   \n",
       "\n",
       "   89517  89518  89519  89520  89521  89522  89523  89524  89525  89526  \n",
       "0      0      0      0      0      0      0      0      0      0      0  \n",
       "1      0      0      0      0      0      0      0      0      0      0  \n",
       "2      0      0      0      0      0      0      0      0      0      0  \n",
       "3      0      0      0      0      0      0      0      0      0      0  \n",
       "4      0      0      0      0      0      0      0      0      0      0  \n",
       "\n",
       "[5 rows x 89527 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=X_train_df[:6000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    138\n",
       "1    114\n",
       "2    115\n",
       "3    104\n",
       "4    232\n",
       "dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(df, columns = [\"Text Length\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Label']=data[\"Text Length\"].apply(lambda x: '1' if x>17 else '0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label\n",
       "0     1\n",
       "1     1\n",
       "2     1\n",
       "3     1\n",
       "4     1"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=data.drop(columns=['Text Length'])\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Label'] = pd.to_numeric(data['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6000 entries, 0 to 5999\n",
      "Data columns (total 1 columns):\n",
      "Label    6000 non-null int64\n",
      "dtypes: int64(1)\n",
      "memory usage: 47.0 KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(max(df), min(df), st.mean(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test1= np.where(y_test <= 4, 0, 1)\n",
    "y_train1=np.where(y_train <= 4, 0, 1)\n",
    "label=y_train1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Target\n",
       "0       1\n",
       "1       1\n",
       "2       1\n",
       "3       1\n",
       "4       1"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label=pd.DataFrame(label, columns = [\"Target\"])\n",
    "label.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6000 entries, 0 to 5999\n",
      "Data columns (total 1 columns):\n",
      "Target    6000 non-null int32\n",
      "dtypes: int32(1)\n",
      "memory usage: 23.5 KB\n"
     ]
    }
   ],
   "source": [
    "label['Target'] = pd.to_numeric(label['Target'])\n",
    "label.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.68\n",
      "Recall:  0.50\n",
      "F1-score:  0.33\n"
     ]
    }
   ],
   "source": [
    "print(\"Precision: %0.2f\" %precision_score(label, data2, average=\"macro\"))\n",
    "print(\"Recall:  %0.2f\" %recall_score(label, data2, average=\"macro\"))\n",
    "print(\"F1-score:  %0.2f\" %f1_score(label, data2 , average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99992"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_precision(label, data2, debug = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_recall(data, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_fscore(data, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tp_tn_fn_fp(y_test1, y_pred):\n",
    "\n",
    "    tp = sum((y_test1 == 1) & (y_pred == 1))\n",
    "    tn = sum((y_test1 == 0) & (y_pred == 0))\n",
    "    fn = sum((y_test1 == 1) & (y_pred == 0))\n",
    "    fp = sum((y_test1 == 0) & (y_pred == 1))\n",
    "    return tp, tn, fp, fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculates the precision of the predicted labels\n",
    "def get_precision(y_pred, y_true, debug = False):\n",
    "    # deal with npdarray\n",
    "    y_pred = list(y_pred)\n",
    "    y_true = list(y_true)\n",
    "    #---------\n",
    "    n = len(y_pred);\n",
    "    y_pred = [0 if v is None else v for v in y_pred]# deal with None type\n",
    "    y_true = [0 if v is None else v for v in y_true]# deal with None type\n",
    "    true_positive = sum(y_pred[i]* y_true[i] for i in range(n))\n",
    "    if (0 == sum(y_pred)): return 0\n",
    "    return true_positive*1.0/sum(y_pred)\n",
    "    \n",
    "## Calculates the recall of the predicted labels\n",
    "def get_recall(y_pred, y_true):\n",
    "    # deal with npdarray\n",
    "    y_pred = list(y_pred)\n",
    "    y_true = list(y_true)\n",
    "    #---------\n",
    "    n = len(y_pred);\n",
    "    y_pred = list(map(int,[1 == l for l in y_pred]))# deal with None type\n",
    "    y_true = list(map(int,[1 == l for l in y_true]))# deal with None type\n",
    "    true_positive = sum(y_pred[i]*y_true[i] for i in range(n))\n",
    "    if 0 == sum(y_true): return 0\n",
    "    return true_positive*1.0/sum(y_true)\n",
    "    \n",
    "\n",
    "## Calculates the f-score of the predicted labels\n",
    "def get_fscore(y_pred, y_true):\n",
    "    precision = get_precision(y_pred, y_true);\n",
    "    if (0 == precision): return 0\n",
    "    recall= get_recall(y_pred, y_true);\n",
    "    if (0 == recall): return 0\n",
    "    beta = 1.0;\n",
    "    # print(\"get_fscore:\",(beta**2*precision+recall))\n",
    "    fscore = (beta**2+1)*precision*recall/(beta**2*precision+recall);\n",
    "    return fscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_length_threshold(df, label, plot_flag = True):\n",
    "    ## YOUR CODE HERE\n",
    "    # print(\"load training file\")\n",
    "    #words,labels = load_file(training_file, True)\n",
    "    Dict_len = defaultdict(set)\n",
    "    for w in df: Dict_len[w].add(w)\n",
    "    # Evaluation depending on fscore\n",
    "    Min = min(Dict_len)\n",
    "    Max = max(Dict_len)\n",
    "    #for Thres in range(Min,Max): \n",
    "            #print(list(map(int,[w < Thres for w in df])))\n",
    "            #print(get_precision(list(map(int,[w < Thres for w in df])), label ) ) \n",
    "    #----------\n",
    "    #yezheng: binary search is possible to improve the serach\n",
    "    #plotting precision vs. recall \n",
    "    ThreRange=  range(Min,Max+1)\n",
    "    FscoreL = dict([(f1_score(list(map(int,[w > Thres for w in df])), label, average=\"macro\"),Thres) for Thres in ThreRange])\n",
    "    Thres_opt = FscoreL[max(FscoreL)]\n",
    "    if plot_flag:\n",
    "         print(\"Range of thresholds:\",Min,\"to\",Max, \" with optimal threshold:\", Thres_opt+1)\n",
    "    #precisionL = [precision_score(list(map(int,[w > Thres for w in df])), label,average=\"macro\")for Thres in ThreRange]\n",
    "    #recallL = [recall_score(list(map(int,[w > Thres for w in df])), label, average=\"macro\") for Thres in ThreRange]   \n",
    "    #Pred_opt = list(map(int,[w> Thres_opt for w in df]))\n",
    "    training_performance = [precision_score(Pred_opt, label, average=\"macro\"), recall_score(Pred_opt, label, average=\"macro\" ), f1_score(Pred_opt, label, average=\"macro\" )]\n",
    "    del df\n",
    "    del label\n",
    "    return ThreRange, Thres_opt, training_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range of thresholds: 12 to 2476  with optimal threshold: 17\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Pred_opt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-59009fc308ff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mword_length_threshold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-11-ec7e348b517c>\u001b[0m in \u001b[0;36mword_length_threshold\u001b[1;34m(df, label, plot_flag)\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;31m#recallL = [recall_score(list(map(int,[w > Thres for w in df])), label, average=\"macro\") for Thres in ThreRange]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;31m#Pred_opt = list(map(int,[w> Thres_opt for w in df]))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[0mtraining_performance\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mprecision_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPred_opt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"macro\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecall_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPred_opt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"macro\"\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPred_opt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"macro\"\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m     \u001b[1;32mdel\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;32mdel\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Pred_opt' is not defined"
     ]
    }
   ],
   "source": [
    "word_length_threshold(df, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2.2: Word length thresholding\n",
    "\n",
    "## Finds the best length threshold by f-score, and uses this threshold to\n",
    "## classify the training and development set\n",
    "def word_length_threshold(training_file, development_file, plot_flag = False):\n",
    "    ## YOUR CODE HERE\n",
    "    # print(\"load training file\")\n",
    "    words,labels = load_file(training_file, True)\n",
    "    Dict_len = defaultdict(set)\n",
    "    for w in words: Dict_len[len(w)].add(w)\n",
    "    # Evaluation depending on fscore\n",
    "    Min = min(Dict_len)\n",
    "    Max = max(Dict_len)\n",
    "    # for Thres in range(Min,Max): \n",
    "    #     # print(list(map(int,[len(w) < Thres for w in words])))\n",
    "    #     print(get_precision(list(map(int,[len(w) < Thres for w in words])), labels ) ) \n",
    "    #----------\n",
    "    #yezheng: binary search is possible to improve the serach\n",
    "    #plotting precision vs. recall \n",
    "    ThreRange =  range(Min,Max+1)\n",
    "    FscoreL = dict([(get_fscore(list(map(int,[len(w) > Thres for w in words])), labels),Thres) for Thres in ThreRange])\n",
    "    Thres_opt = FscoreL[max(FscoreL)]\n",
    "    if plot_flag:\n",
    "        print(\"Range of thresholds:\",Min,\"to\",Max, \" with optimal threshold:\", Thres_opt+1)\n",
    "        precisionL = [get_precision(list(map(int,[len(w) > Thres for w in words])), labels)for Thres in ThreRange]\n",
    "        recallL = [get_recall(list(map(int,[len(w) > Thres for w in words])), labels) for Thres in ThreRange]\n",
    "        Ret_plot = [precisionL,recallL]\n",
    "        plt.plot(recallL, precisionL,'^r', label = 'Train')\n",
    "        plt.title('word_length_threshold: precision-recall curve')\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "\n",
    "    Pred_opt = list(map(int,[len(w)> Thres_opt for w in words]))\n",
    "    training_performance = [get_precision(Pred_opt, labels), get_recall(Pred_opt, labels), get_fscore(Pred_opt, labels)]\n",
    "    del words\n",
    "    del labels\n",
    "    # print(\"load development file\")\n",
    "    words,labels = load_file(development_file)\n",
    "    Pred_opt = list(map(int,[len(w)> Thres_opt for w in words]))\n",
    "    if plot_flag: \n",
    "        precisionL = [get_precision(list(map(int,[len(w) > Thres for w in words])), labels)for Thres in ThreRange]\n",
    "        recallL = [get_recall(list(map(int,[len(w) > Thres for w in words])), labels) for Thres in ThreRange]\n",
    "        plt.plot(recallL, precisionL,'^g', label = 'Dev')\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.show()\n",
    "        Ret_plot += [precisionL,recallL]\n",
    "    development_performance = [get_precision(Pred_opt, labels), get_recall(Pred_opt, labels), get_fscore(Pred_opt, labels)]\n",
    "    if plot_flag: return training_performance, development_performance,Ret_plot\n",
    "    return training_performance, development_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
